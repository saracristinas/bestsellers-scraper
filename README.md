# ğŸ“Œ Amazon Bestsellers Scraper API

Este projeto Ã© uma API Serverless que utiliza um web scraper para capturar os produtos mais vendidos da Amazon Brasil. Os dados extraÃ­dos sÃ£o armazenados no DynamoDB e podem ser acessados por meio de um endpoint HTTP. O scraper roda localmente e alimenta o banco de dados na AWS, enquanto a API Ã© hospedada na AWS utilizando Lambda e API Gateway.

---

## ğŸ“‹ SumÃ¡rio
1. [âœ… Requisitos ObrigatÃ³rios](#-requisitos-obrigatÃ³rios)
2. [ğŸ› ï¸ Tecnologias Utilizadas](#%EF%B8%8F-tecnologias-utilizadas)
3. [ğŸš€ Como Rodar Localmente](#-como-rodar-localmente)
4. [ğŸŒ Endpoint da API em ProduÃ§Ã£o](#-endpoint-da-api-em-produÃ§Ã£o)
5. [ğŸ“„ Endpoints DisponÃ­veis](#-endpoints-disponÃ­veis)
6. [ğŸ” AutenticaÃ§Ã£o](#-autenticaÃ§Ã£o)
7. [ğŸ“ Sobre o Desenvolvimento](#-sobre-o-desenvolvimento)
8. [ğŸ¯ PrÃ³ximos Passos](#-prÃ³ximos-passos)
9. [ğŸ“ Contato](#-contato)

---

## âœ… Requisitos ObrigatÃ³rios

âœ” Criar um scraper para pegar os produtos mais vendidos da Amazon  
âœ” Salvar os produtos no DynamoDB  
âœ” Criar uma API que retorna os produtos armazenados no DynamoDB  
âœ” Usar AWS Lambda para processar as requisiÃ§Ãµes  
âœ” Usar API Gateway para expor a API  
âœ” Usar Serverless Framework para gerenciar a infraestrutura  
âœ” Usar Node.js (sem TypeScript)  
âœ” Criar commits organizados para mostrar seu processo de trabalho  

---

## ğŸ› ï¸ Tecnologias Utilizadas

- **Node.js**  
- **Puppeteer** (para scraping)  
- **AWS Lambda** (para funÃ§Ãµes serverless)  
- **AWS API Gateway** (para expor os endpoints)  
- **AWS DynamoDB** (banco de dados NoSQL)  
- **Serverless Framework** (para gerenciar a infraestrutura)  

---

## ğŸš€ Como Rodar o Projeto

### ğŸ“Œ **Se quiser apenas acessar os produtos jÃ¡ salvos na AWS:**

Se vocÃª deseja **somente consultar os produtos** sem rodar nada localmente, basta acessar a API jÃ¡ hospedada na AWS:

```sh
curl -X GET https://apmou89x9b.execute-api.us-east-1.amazonaws.com/products
```

Isso retornarÃ¡ os produtos que jÃ¡ estÃ£o armazenados no **DynamoDB na AWS**.

---

### ğŸ“Œ **Rodando o Scraper Localmente (para popular a AWS)**

Se vocÃª quiser **rodar o scraper e adicionar mais produtos ao banco na AWS**, siga estes passos:

1ï¸âƒ£ **Clonar o repositÃ³rio:**

```sh
git clone https://github.com/saracristinas/bestsellers-scraper.git
```

2ï¸âƒ£ **Instalar dependÃªncias:**

```sh
npm install
```

3ï¸âƒ£ **Rodar o scraper para popular o banco na AWS:**

```sh
node scraper.js
```

Agora os produtos foram adicionados ao DynamoDB na AWS e podem ser acessados pela API online.

---

### ğŸ“Œ **Rodando a API Localmente**

Se quiser testar a API **localmente**, Ã© necessÃ¡rio instalar o **Serverless Framework**.

1ï¸âƒ£ **Instalar o Serverless Framework:**

```sh
npm install -g serverless
```

2ï¸âƒ£ **Rodar a API localmente:**

```sh
serverless offline
```

Agora os endpoints estarÃ£o disponÃ­veis localmente, por exemplo:

```sh
curl -X GET http://localhost:3000/products
```

âš  **Se estiver usando DynamoDB localmente**, vocÃª precisa rodar o banco antes:

```sh
serverless dynamodb start
```

Se nÃ£o rodar o scraper, a API local **retornarÃ¡ uma lista vazia**.

---

## ğŸ“ Sobre o Desenvolvimento

Este projeto foi desenvolvido para demonstrar habilidades em **web scraping, AWS e Serverless Framework**. Durante o desenvolvimento, enfrentei desafios como otimizaÃ§Ã£o do Puppeteer para rodar em ambientes restritos e configuraÃ§Ã£o de infraestrutura serverless.

A principal decisÃ£o foi manter a API **simples e direta**, sem adicionar camadas desnecessÃ¡rias. A API funciona sem necessidade de autenticaÃ§Ã£o e os dados sÃ£o extraÃ­dos diretamente da Amazon com um identificador real do produto.

---
## ğŸ“„ Endpoints DisponÃ­veis

### ğŸ”¹ `GET /products`
Retorna os produtos armazenados no DynamoDB.

**ParÃ¢metros Opcionais:**
- `quantity` (nÃºmero de produtos a serem retornados, ex: `3`, `5`, `10`)

**Exemplo de RequisiÃ§Ã£o:**
```sh
curl -X GET https://apmou89x9b.execute-api.us-east-1.amazonaws.com/products?quantity=5
```

---

## ğŸ” AutenticaÃ§Ã£o
Atualmente, nÃ£o hÃ¡ autenticaÃ§Ã£o na API, pois o scraper roda localmente. No futuro, medidas de seguranÃ§a poderÃ£o ser implementadas.

---

## ğŸŒ Endpoint da API em ProduÃ§Ã£o

A API estÃ¡ disponÃ­vel na AWS e pode ser acessada pelo seguinte endpoint:

ğŸ”— https://apmou89x9b.execute-api.us-east-1.amazonaws.com/products

---

## ğŸ“ Sobre o Desenvolvimento

Ao iniciar este projeto, minha principal preocupaÃ§Ã£o foi estruturar o cÃ³digo de forma clara e objetiva. Inicialmente, considerei dividir o cÃ³digo em controllers, services e routes, mas percebi que para um projeto desse porte, manter a estrutura simples e direta era a melhor escolha.

No inÃ­cio, tive desafios ao configurar o Serverless Framework, principalmente na configuraÃ§Ã£o das credenciais da AWS. A documentaÃ§Ã£o nem sempre era clara, e foi preciso bastante tentativa e erro para conseguir fazer o deploy corretamente.

Outro grande desafio foi o Puppeteer. Ele Ã© pesado e nÃ£o funciona diretamente na AWS Lambda devido ao limite de tamanho. Para resolver isso, utilizei `chrome-aws-lambda` e `puppeteer-core`, que sÃ£o versÃµes otimizadas para ambientes serverless. Essa adaptaÃ§Ã£o foi um dos momentos mais desafiadores do projeto.

A extraÃ§Ã£o dos dados do site da Amazon tambÃ©m exigiu atenÃ§Ã£o. Utilizei `querySelector` para capturar os elementos do DOM e implementei regex para extrair o ID real dos produtos a partir da URL. Dessa forma, a API consegue manter os dados organizados e confiÃ¡veis.

Por fim, para a API, inicialmente pensei em utilizar Express.js, mas percebi que o API Gateway jÃ¡ faz esse trabalho, tornando essa dependÃªncia desnecessÃ¡ria. Removi para deixar o projeto mais leve e eficiente.

O resultado final Ã© uma API funcional e bem estruturada, atendendo aos requisitos do desafio e proporcionando uma base sÃ³lida para futuras melhorias.

---
## ğŸ¯ PrÃ³ximos Passos

- **Adicionar novos endpoints** para mais funcionalidades.
- **Melhorar o scraper** para garantir que colete os dados corretamente.
- **Implementar autenticaÃ§Ã£o** para proteger a API.
- **Melhorar logs e monitoramento** para identificar erros mais facilmente.

---

## ğŸ¯ ConsideraÃ§Ãµes Finais

Esse projeto foi um grande desafio para mim, principalmente porque precisei aprender muitas coisas novas em pouco tempo. Desde o Serverless Framework atÃ© a otimizaÃ§Ã£o do Puppeteer para rodar na AWS, cada etapa foi uma jornada de aprendizado.

Apesar de nÃ£o estar 100% perfeito e ter alguns detalhes que poderiam ser refinados, estou muito orgulhosa do que consegui entregar. Foram longos dias estudando, testando, errando e tentando novamente, mas valeu a pena. A experiÃªncia de desenvolver essa API me ensinou muito sobre arquitetura serverless, scraping e AWS, e sei que tudo isso serÃ¡ Ãºtil nos meus prÃ³ximos projetos.

No fim, o mais importante foi ter concluÃ­do o desafio e aprendido tanto no processo. Cada erro e acerto me ajudaram a evoluir como desenvolvedora, e isso Ã© o que mais me motiva a continuar.

---


## ğŸ“ Contato
Caso tenha alguma dÃºvida ou sugestÃ£o, entre em contato!

ğŸ‘©â€ğŸ’» **Desenvolvedora:** [Sara Cristina](https://github.com/saracristinas)  
ğŸ“§ **Email:** [sarasales17062000@gmail.com](mailto:sarasales17062000@gmail.com)




