# ğŸ“Œ Amazon Bestsellers Scraper API

Este projeto Ã© uma API Serverless que utiliza um web scraper para capturar os produtos mais vendidos da Amazon Brasil. Os dados extraÃ­dos sÃ£o armazenados no DynamoDB e podem ser acessados por meio de um endpoint HTTP. O scraper roda localmente e alimenta o banco de dados na AWS, enquanto a API Ã© hospedada na AWS utilizando Lambda e API Gateway.

---

## ğŸ“‹ SumÃ¡rio
1. [âœ… Requisitos ObrigatÃ³rios](#-requisitos-obrigatÃ³rios)
2. [ğŸ› ï¸ Tecnologias Utilizadas](#%EF%B8%8F-tecnologias-utilizadas)
3. [ğŸš€ Como Rodar Localmente](#-como-rodar-localmente)
4. [ğŸŒ Endpoint da API em ProduÃ§Ã£o](#-endpoint-da-api-em-produÃ§Ã£o)
5. [ğŸ“„ Endpoints DisponÃ­veis](#-endpoints-disponÃ­veis)
6. [ğŸ” AutenticaÃ§Ã£o](#-autenticaÃ§Ã£o)
7. [ğŸ“ Sobre o Desenvolvimento](#-sobre-o-desenvolvimento)
8. [ğŸ¯ ConsideraÃ§Ãµes Finais](#-consideraÃ§Ãµes-finais)
9. [ğŸ“ Contato](#-contato)

---

## âœ… Requisitos ObrigatÃ³rios

âœ” Criar um scraper para pegar os produtos mais vendidos da Amazon  
âœ” Salvar os produtos no DynamoDB  
âœ” Criar uma API que retorna os produtos armazenados no DynamoDB  
âœ” Usar AWS Lambda para processar as requisiÃ§Ãµes  
âœ” Usar API Gateway para expor a API  
âœ” Usar Serverless Framework para gerenciar a infraestrutura  
âœ” Usar Node.js (sem TypeScript)  
âœ” Criar commits organizados para mostrar seu processo de trabalho  

---

## ğŸ› ï¸ Tecnologias Utilizadas

- **Node.js**  
- **Puppeteer** (para scraping)  
- **AWS Lambda** (para funÃ§Ãµes serverless)  
- **AWS API Gateway** (para expor os endpoints)  
- **AWS DynamoDB** (banco de dados NoSQL)  
- **Serverless Framework** (para gerenciar a infraestrutura)  

---

## ğŸš€ Como Rodar Localmente

### 1ï¸âƒ£ PrÃ©-requisitos
Antes de comeÃ§ar, **certifique-se** de ter instalado:

- **Node.js** (v20 ou superior)  
- **NPM** ou **Yarn**  
- **AWS CLI** configurado (caso queira testar com DynamoDB na AWS)  
- **Serverless Framework** instalado globalmente:  
  ```sh
  npm install -g serverless
  ```

### 2ï¸âƒ£ Instalar DependÃªncias
No diretÃ³rio do projeto, execute:  
```sh
npm install
```

### 3ï¸âƒ£ Rodar o DynamoDB Localmente *(Opcional)*
Se vocÃª deseja testar sem precisar da AWS, pode rodar o DynamoDB localmente:  
```sh
serverless dynamodb start
```
âš ï¸ **ObservaÃ§Ã£o:** Certifique-se de que o plugin `serverless-dynamodb-local` estÃ¡ instalado e configurado no `serverless.yml`. Se ainda nÃ£o estiver, instale com:  
```sh
npm install --save-dev serverless-dynamodb-local
```

### 4ï¸âƒ£ Rodar a API Localmente
Execute o seguinte comando para iniciar a API:  
```sh
serverless offline --reloadHandler
```
Isso disponibilizarÃ¡ os endpoints localmente.  

âš ï¸ **Se o comando nÃ£o funcionar**, tente:  
```sh
serverless offline
```

---

## ğŸŒ Endpoint da API em ProduÃ§Ã£o

A API estÃ¡ disponÃ­vel na AWS e pode ser acessada pelo seguinte endpoint:

ğŸ”— https://apmou89x9b.execute-api.us-east-1.amazonaws.com/products

---
## ğŸ“„ Endpoints DisponÃ­veis

### ğŸ”¹ `GET /products`
Retorna os produtos armazenados no DynamoDB.

**ParÃ¢metros Opcionais:**
- `quantity` (nÃºmero de produtos a serem retornados, ex: `3`, `5`, `10`)

**Exemplo de RequisiÃ§Ã£o:**
```sh
curl -X GET https://apmou89x9b.execute-api.us-east-1.amazonaws.com/products?quantity=5
```

### ğŸ”¹ `POST /scrape`
Executa o scraper manualmente para buscar novos produtos e armazenÃ¡-los no DynamoDB.

**Exemplo de RequisiÃ§Ã£o:**
```sh
curl -X POST https://apmou89x9b.execute-api.us-east-1.amazonaws.com/scrape
```

---

## ğŸ” AutenticaÃ§Ã£o
Para acessar o serviÃ§o localmente, vocÃª precisarÃ¡ configurar as credenciais da AWS na sua mÃ¡quina.

---

## ğŸ“ Sobre o Desenvolvimento

Quando comecei esse projeto, minha primeira ideia foi estruturar o cÃ³digo separando controllers, services e routes. No entanto, com o tempo, percebi que essa divisÃ£o acabava sendo desnecessÃ¡ria para um projeto desse porte. Decidi entÃ£o simplificar a organizaÃ§Ã£o, deixando tudo mais direto e fÃ¡cil de manter.

No inÃ­cio, tambÃ©m enfrentei dificuldades com o Serverless Framework. Eu nÃ£o conhecia muito bem a ferramenta e, alÃ©m disso, precisei aprender sobre as credenciais da AWS para conseguir fazer o deploy corretamente. Confesso que esse foi um dos momentos mais desafiadores, pois os erros relacionados Ã  configuraÃ§Ã£o das credenciais nÃ£o eram muito claros no comeÃ§o.

Outro problema grande foi lidar com as dependÃªncias do projeto. O Puppeteer, por exemplo, deu muito trabalho. Ele Ã© pesado, e ao tentar rodÃ¡-lo na AWS Lambda, descobri que ultrapassava o limite de tamanho permitido. Depois de algumas pesquisas, encontrei uma soluÃ§Ã£o: usar chrome-aws-lambda (uma versÃ£o reduzida do Chrome) e puppeteer-core, que Ã© uma versÃ£o mais leve da biblioteca. Foi um aprendizado e tanto, mas no final consegui fazer com que o scraper rodasse corretamente.

Depois que o scraping estava funcionando, precisei garantir que os dados fossem extraÃ­dos corretamente. Usei querySelector para capturar informaÃ§Ãµes do DOM da Amazon e ainda implementei regex para extrair o ID real dos produtos a partir da URL. Isso garantiu que os dados fossem coletados de maneira confiÃ¡vel, sem depender apenas da estrutura visual da pÃ¡gina, que pode mudar com o tempo.

Com o scraper pronto, passei para a parte da API. Inicialmente, pensei em usar Express.js, mas depois percebi que isso era desnecessÃ¡rio, jÃ¡ que o API Gateway jÃ¡ cuida do roteamento. Acabei removendo essa dependÃªncia para deixar o projeto mais leve e otimizado.

No final, apesar dos desafios, fiquei satisfeita com o resultado. O projeto agora estÃ¡ funcional, simples e cumpre bem seu objetivo. Ainda hÃ¡ espaÃ§o para melhorias, mas ele jÃ¡ atende completamente aos requisitos do desafio!

---
## ğŸ¯ PrÃ³ximos Passos

Melhorar o funcionamento do scraper para garantir que colete os dados corretamente.

Implementar autenticaÃ§Ã£o para proteger a API.

Melhorar logs e monitoramento para identificar erros mais facilmente.

---

## ğŸ¯ ConsideraÃ§Ãµes Finais

Esse projeto foi um grande desafio para mim, principalmente porque precisei aprender muitas coisas novas em pouco tempo. Desde o Serverless Framework atÃ© a otimizaÃ§Ã£o do Puppeteer para rodar na AWS, cada etapa foi uma jornada de aprendizado.

Apesar de nÃ£o estar 100% perfeito e ter alguns detalhes que poderiam ser refinados, estou muito orgulhosa do que consegui entregar. Foram longos dias estudando, testando, errando e tentando novamente, mas valeu a pena. A experiÃªncia de desenvolver essa API me ensinou muito sobre arquitetura serverless, scraping e AWS, e sei que tudo isso serÃ¡ Ãºtil nos meus prÃ³ximos projetos.

No fim, o mais importante foi ter concluÃ­do o desafio e aprendido tanto no processo. Cada erro e acerto me ajudaram a evoluir como desenvolvedora, e isso Ã© o que mais me motiva a continuar.

---


## ğŸ“ Contato
Caso tenha alguma dÃºvida ou sugestÃ£o, entre em contato!

ğŸ‘©â€ğŸ’» **Desenvolvedora:** [Sara Cristina](https://github.com/saracristinas)  
ğŸ“§ **Email:** [sarasales17062000@gmail.com](mailto:sarasales17062000@gmail.com)




